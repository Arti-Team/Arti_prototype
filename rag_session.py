ã„·w#!/usr/bin/env python3
"""
RAG Session Brief Generator

Generates evidence-based curation briefs by querying research papers
and creating structured recommendations for artwork selection.

Usage:
    from rag_session import RAGSessionBrief
    
    brief_gen = RAGSessionBrief()
    brief = brief_gen.generate_brief("feeling stressed at work", ["anxiety", "overwhelmed"])
"""

import os
import json
import hashlib
from pathlib import Path
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# LlamaIndex imports
from llama_index.core import StorageContext, load_index_from_storage
from llama_index.vector_stores.faiss import FaissVectorStore
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.openai import OpenAI as LlamaOpenAI
from llama_index.core import Settings

import faiss

class RAGSessionBrief:
    """
    RAG Session Brief Generator
    
    Combines research evidence with LLM reasoning to generate
    personalized curation briefs for artwork recommendation.
    """
    
    def __init__(self, index_path: str = None, cache_dir: str = None):
        # Use relative paths if no specific path provided
        script_dir = Path(__file__).parent
        self.index_path = Path(index_path) if index_path else script_dir / "indices" / "rag_faiss"
        self.model_dir = script_dir / "models" / "all-MiniLM-L6-v2"
        self.cache_dir = Path(cache_dir) if cache_dir else script_dir / ".cache" / "rag_sessions"
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # Initialize components
        self.query_engine = None
        self.llm = None
        self._setup_rag_system()
        self._setup_llm()
        
    def _setup_rag_system(self):
        """Load the persisted RAG index and setup query engine"""
        print("ğŸ”„ Loading RAG index...")
        
        # Setup embedding model with relative path
        embed_model = HuggingFaceEmbedding(
            model_name=str(self.model_dir)
        )
        
        # Set global settings
        Settings.embed_model = embed_model
        
        # Create empty FAISS vector store - this will be populated from storage
        faiss_index = faiss.IndexFlatL2(384)  # all-MiniLM-L6-v2 dimension
        vector_store = FaissVectorStore(faiss_index=faiss_index)
        
        # Load storage context from persisted directory
        storage_context = StorageContext.from_defaults(
            vector_store=vector_store,
            persist_dir=str(self.index_path)
        )
        
        # Load the index with proper embedding model
        index = load_index_from_storage(storage_context, embed_model=embed_model)
        
        # Create query engine
        self.query_engine = index.as_query_engine(
            similarity_top_k=10,
            response_mode="no_text"  # We only want the source nodes
        )
        
        print("âœ… RAG index loaded successfully")
        
    def _setup_llm(self):
        """Setup LLM for brief generation"""
        # Use Fireworks API like in test_rag_performance.py
        fireworks_key = os.getenv("FIREWORKS_API_KEY")
        
        if fireworks_key and fireworks_key != "your_fireworks_api_key_here":
            self.llm = LlamaOpenAI(
                model="gpt-4o",
                api_key=fireworks_key,
                api_base="https://api.fireworks.ai/inference/v1",
                temperature=0.1
            )
            print("âœ… LLM setup complete (Fireworks)")
            return
        
        # Fallback to OpenAI
        openai_key = os.getenv("OPENAI_API_KEY")
        if openai_key and openai_key != "your-api-key-here":
            self.llm = LlamaOpenAI(
                model="gpt-4o-mini",
                temperature=0.1,
                api_key=openai_key
            )
            print("âœ… LLM setup complete (OpenAI)")
            return
            
        print("âš ï¸ Warning: No valid API key found for LLM")
        
    def make_rag_queries(self, situation: str, emotions: List[str]) -> List[str]:
        """Generate targeted RAG queries based on user situation and emotions"""
        emotion_str = ', '.join(emotions)
        
        base_queries = [
            f"ìƒí™©: {situation}. ê°ì •: {emotion_str}. ê°ì„± ì €ê°ì— ì í•©í•œ ìƒ‰ì±„(ëª…ë„/ì±„ë„/ìƒ‰ìƒ) ê°€ì´ë“œ ìš”ì•½",
            "Ecological Valence Theory í•µì‹¬ê³¼ íŒŒë‘/ì´ˆë¡ ì„ í˜¸ì˜ ê·¼ê±°",
            "ë¶ˆì•ˆ/ë¶„ë…¸ ê³ ê°ì„±ì—ì„œ í”¼í•´ì•¼ í•  ì‹œê° ìê·¹(ìƒ‰ ëŒ€ë¹„/ì£¼ì œ) ê·¼ê±°", 
            "ìì—°/ë¸”ë£¨ìŠ¤í˜ì´ìŠ¤(ë¬¼ê°€/í•˜ëŠ˜)ì˜ ì •ì„œ íš¨ê³¼ ê·¼ê±°",
            "ë¯¸ìˆ ì¹˜ë£Œ/ìì—° ì´ë¯¸ì§€ ì¤‘ì¬ì˜ íš¨ê³¼(ë©”íƒ€ë¶„ì„/ë¦¬ë·°) í•µì‹¬ ìˆ˜ì¹˜"
        ]
        
        # Add emotion-specific queries
        if "anxiety" in emotions or "ë¶ˆì•ˆ" in emotions:
            base_queries.append("ë¶ˆì•ˆ ê°ì†Œë¥¼ ìœ„í•œ ìƒ‰ì±„ ì¹˜ë£Œ ì—°êµ¬ ê²°ê³¼")
            base_queries.append("ê³ ì±„ë„ ìƒ‰ìƒì´ ë¶ˆì•ˆì— ë¯¸ì¹˜ëŠ” ë¶€ì •ì  ì˜í–¥")
            
        if "stress" in emotions or "ìŠ¤íŠ¸ë ˆìŠ¤" in emotions:
            base_queries.append("ìŠ¤íŠ¸ë ˆìŠ¤ ì™„í™”ë¥¼ ìœ„í•œ ìì—° ê²½ê´€ ì´ë¯¸ì§€ì˜ íš¨ê³¼")
            base_queries.append("ì—…ë¬´ ìŠ¤íŠ¸ë ˆìŠ¤ì™€ ìƒ‰ì±„ í™˜ê²½ì˜ ìƒê´€ê´€ê³„")
            
        if "depression" in emotions or "ìš°ìš¸" in emotions:
            base_queries.append("ìš°ìš¸ì¦ì— ëŒ€í•œ ë¯¸ìˆ ì¹˜ë£Œ ë©”íƒ€ë¶„ì„ ê²°ê³¼")
            base_queries.append("ë°ì€ ìƒ‰ìƒê³¼ ê¸°ë¶„ ê°œì„ ì˜ ìƒê´€ê´€ê³„")
            
        return base_queries
    
    def fetch_evidence(self, queries: List[str], top_k: int = 3) -> List[Dict[str, Any]]:
        """Fetch evidence from RAG system for given queries"""
        if not self.query_engine:
            raise RuntimeError("RAG system not initialized")
            
        print(f"ğŸ”„ Fetching evidence for {len(queries)} queries...")
        
        evidence_list = []
        for i, query in enumerate(queries):
            print(f"  Query {i+1}/{len(queries)}: {query[:50]}...")
            
            try:
                response = self.query_engine.query(query)
                
                for source_node in response.source_nodes[:top_k]:
                    metadata = source_node.node.metadata
                    
                    evidence = {
                        "title": metadata.get("title", "Unknown"),
                        "year": metadata.get("year", "Unknown"),
                        "domain": metadata.get("domain", "Unknown"),
                        "score": float(source_node.score) if source_node.score else 0.0,
                        "snippet": source_node.node.get_content()[:650].replace("\n", " "),
                        "query_used": query
                    }
                    evidence_list.append(evidence)
                    
            except Exception as e:
                print(f"    âš ï¸ Error with query: {e}")
                continue
        
        # Remove duplicates based on title and snippet start
        unique_evidence = {}
        for evidence in evidence_list:
            key = (evidence["title"], evidence["snippet"][:120])
            if key not in unique_evidence:
                unique_evidence[key] = evidence
                
        final_evidence = list(unique_evidence.values())[:12]  # Limit to top 12
        print(f"âœ… Collected {len(final_evidence)} unique evidence pieces")
        
        return final_evidence
    
    def render_brief_prompt(self, evidence: List[Dict[str, Any]], situation: str, emotions: List[str]) -> str:
        """Generate prompt for LLM brief generation"""
        evidence_text = "\n".join([
            f"- ({e['title']} {e.get('year')}) {e['snippet']}" 
            for e in evidence
        ])
        
        prompt = f"""ë‹¹ì‹ ì€ ê·¼ê±° ì¤‘ì‹¬ ë¯¸ìˆ  íë ˆì´í„°ì…ë‹ˆë‹¤.
ì•„ë˜ evidenceë§Œ ê·¼ê±°ë¡œ, ì˜¤ëŠ˜ ì‚¬ìš©ì ìƒí™©ì— ë§ëŠ” ì„¸ì…˜ ë¸Œë¦¬í”„ë¥¼ JSONìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.

ì…ë ¥:
- user_situation_summary: "{situation}"
- user_emotion: {emotions}

ì¶œë ¥(JSON):
{{
  "curatorial_goals": [ "3~5ê°œì˜ íë ˆì´ì…˜ ëª©í‘œ" ],
  "desired_themes": [ "5~8ê°œì˜ ì›í•˜ëŠ” ì£¼ì œ" ],
  "avoid_today": [ "ì˜¤ëŠ˜ í”¼í•´ì•¼ í•  ìš”ì†Œë“¤" ],
  "palette_direction": {{"hue": ["blue","green"], "saturation":"low", "value":"high"}},
  "style_tempo": ["impressionist","soft edges","slow tempo"],
  "citations": [{{"title":"...", "year":"..."}}]
}}

ì œì•½:
- evidence ë°– ì¶”ì¸¡ ê¸ˆì§€, JSONë§Œ ë°˜í™˜
- curatorial_goals: ì‚¬ìš©ì ê°ì • ìƒíƒœ ê°œì„ ì„ ìœ„í•œ êµ¬ì²´ì  ëª©í‘œ
- desired_themes: water/sky/garden/companionship/nature ë“± êµ¬ì²´ì  ì£¼ì œ
- avoid_today: í˜„ì¬ ê°ì • ìƒíƒœì— ë¶€ì ì ˆí•œ ì£¼ì œë‚˜ ìƒ‰ìƒ
- palette_direction: HSV ê¸°ë°˜ ìƒ‰ìƒ ê°€ì´ë“œë¼ì¸
- style_tempo: í™”í’ê³¼ ë¦¬ë“¬ê° ê°€ì´ë“œë¼ì¸
- citations: ì‚¬ìš©ëœ ì—°êµ¬ ë…¼ë¬¸ ì¶œì²˜

evidence:
<<<
{evidence_text}
>>>"""
        return prompt
    
    def call_llm(self, prompt: str) -> Dict[str, Any]:
        """Call LLM and parse JSON response"""
        if not self.llm:
            raise RuntimeError("LLM not initialized - check OPENAI_API_KEY")
            
        try:
            response = self.llm.complete(prompt)
            response_text = response.text.strip()
            
            # Try to extract JSON from response
            if "```json" in response_text:
                # Extract JSON from code block
                start = response_text.find("```json") + 7
                end = response_text.find("```", start)
                response_text = response_text[start:end].strip()
            elif response_text.startswith("```"):
                # Remove code block markers
                response_text = response_text.strip("`").strip()
                
            # Parse JSON
            brief_json = json.loads(response_text)
            return brief_json
            
        except json.JSONDecodeError as e:
            print(f"âŒ JSON parsing error: {e}")
            print(f"Response was: {response_text[:200]}...")
            raise
        except Exception as e:
            print(f"âŒ LLM call error: {e}")
            raise
    
    def _get_cache_key(self, situation: str, emotions: List[str]) -> str:
        """Generate cache key for situation and emotions"""
        content = f"{situation}:{':'.join(sorted(emotions))}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def _load_from_cache(self, cache_key: str) -> Optional[Dict[str, Any]]:
        """Load result from cache if exists"""
        cache_file = self.cache_dir / f"{cache_key}.json"
        if cache_file.exists():
            with open(cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return None
    
    def _save_to_cache(self, cache_key: str, result: Dict[str, Any]):
        """Save result to cache"""
        cache_file = self.cache_dir / f"{cache_key}.json"
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
    
    def generate_brief(self, situation: str, emotions: List[str], use_cache: bool = True) -> Dict[str, Any]:
        """
        Generate complete session brief
        
        Args:
            situation: User situation description
            emotions: List of user emotions
            use_cache: Whether to use caching
            
        Returns:
            Complete session brief with evidence and citations
        """
        print(f"\nğŸ¯ Generating session brief...")
        print(f"   Situation: {situation}")
        print(f"   Emotions: {emotions}")
        
        # Check cache first
        cache_key = self._get_cache_key(situation, emotions)
        if use_cache:
            cached = self._load_from_cache(cache_key)
            if cached:
                print("âœ… Using cached result")
                return cached
        
        # Step 1: Generate RAG queries
        queries = self.make_rag_queries(situation, emotions)
        print(f"ğŸ“ Generated {len(queries)} RAG queries")
        
        # Step 2: Fetch evidence
        evidence = self.fetch_evidence(queries)
        
        # Step 3: Generate brief prompt
        prompt = self.render_brief_prompt(evidence, situation, emotions)
        
        # Step 4: Call LLM
        print("ğŸ¤– Calling LLM to generate brief...")
        brief_json = self.call_llm(prompt)
        
        # Step 5: Package complete result
        result = {
            "brief": brief_json,
            "evidence_used": evidence,
            "queries_used": queries,
            "user_input": {
                "situation": situation,
                "emotions": emotions
            },
            "metadata": {
                "cache_key": cache_key,
                "evidence_count": len(evidence)
            }
        }
        
        # Cache result
        if use_cache:
            self._save_to_cache(cache_key, result)
            
        print("âœ… Session brief generated successfully!")
        return result

def main():
    """Example usage"""
    # Initialize brief generator
    brief_gen = RAGSessionBrief()
    
    # Example 1: Work stress
    print("\n" + "="*60)
    print("Example 1: Work Stress Scenario")
    print("="*60)
    
    result1 = brief_gen.generate_brief(
        situation="ì—…ë¬´ë¡œ ì¸í•œ ìŠ¤íŠ¸ë ˆìŠ¤ê°€ ì‹¬í•˜ê³  ì§‘ì¤‘ì´ ì–´ë ¤ìš´ ìƒí™©",
        emotions=["stress", "anxiety", "overwhelmed"]
    )
    
    print("\nğŸ“‹ Generated Brief:")
    print(json.dumps(result1["brief"], ensure_ascii=False, indent=2))
    
    # Example 2: Evening relaxation
    print("\n" + "="*60)
    print("Example 2: Evening Relaxation Scenario") 
    print("="*60)
    
    result2 = brief_gen.generate_brief(
        situation="í•˜ë£¨ ì¼ê³¼ë¥¼ ë§ˆì¹˜ê³  ì§‘ì—ì„œ íœ´ì‹ì„ ì·¨í•˜ê³  ì‹¶ì€ ì €ë… ì‹œê°„",
        emotions=["tired", "peaceful", "contemplative"]
    )
    
    print("\nğŸ“‹ Generated Brief:")
    print(json.dumps(result2["brief"], ensure_ascii=False, indent=2))
    
    print(f"\nğŸ’¾ Results cached in: {brief_gen.cache_dir}")

if __name__ == "__main__":
    main()